{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Moviefy.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavideEva/Moviefy/blob/main/Moviefy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cIJV64OLD8N"
      },
      "source": [
        "# Problem analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2A9K_1ajnk6i"
      },
      "source": [
        "# Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4WdF4y4poYid"
      },
      "source": [
        "from glob import glob\n",
        "flickr30k_folder = 'Flickr30k-images-preprocessed'\n",
        "! rm -rf sample_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHUU6tAJDObT"
      },
      "source": [
        "! gdown --id \"10c0Xruu2wAE-FpQEIlXm17HlwQAJKKVM\"\n",
        "! mkdir \"$flickr30k_folder\"\n",
        "! unzip -qo flickr30k-images-preprocessed.zip -d \"$flickr30k_folder\"\n",
        "! rm flickr30k-images-preprocessed.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daFFC2I7nmIK"
      },
      "source": [
        "films_data = {\n",
        "    \"Ghibli\": \"1bR_BE-ZZSXW1URBJqPIMFo9VLODYb4_k\"\n",
        "}\n",
        "ghibli_index = 0\n",
        "real_folder = 'real'\n",
        "smooth_folder = 'smooth'\n",
        "\n",
        "def load_data(id, name):\n",
        "  ! mkdir -p \"$name\"\n",
        "  ! cd \"$name\" && gdown --id \"$id\"\n",
        "  zip_files = glob(f'{name}/*.zip')\n",
        "  for zip_file in zip_files:\n",
        "    ! unzip -qo \"$zip_file\" -d \"$name\"\n",
        "    ! rm \"$zip_file\"\n",
        "  return name\n",
        "\n",
        "folders = [load_data(id_drive, studio_name) for studio_name, id_drive in films_data.items()]\n",
        "print(folders)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5tDATAlpXuL"
      },
      "source": [
        "Folder structure:\n",
        "```bash\n",
        ".\n",
        "├── Studio_Name\n",
        "|   ├── real\n",
        "|   |   └── Movie_Name_1\n",
        "|   |       ├── Scene-1\n",
        "|   |       |   ├── left\n",
        "|   |       |   |   ├── 0.jpg\n",
        "|   |       |   |   ...\n",
        "|   |       |   |   └──\n",
        "|   |       |   └── right\n",
        "|   |       |       ├── 0.jpg\n",
        "|   |       |       ...\n",
        "|   |       |       └──\n",
        "|   |       ├── Scene-2\n",
        "|   |       ...\n",
        "|   |       └──\n",
        "|   |   \n",
        "|   └── smooth\n",
        "|       └── Movie_Name_2\n",
        "|           ├── Scene-1\n",
        "|           |   ├── left\n",
        "|           |   └── right\n",
        "|           ├── Scene-2\n",
        "|           ...\n",
        "|           └──\n",
        "├── Studio_Name_2\n",
        "...\n",
        "└──\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXtTLjUKToxO"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHYnk7xOGrwh"
      },
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import random\n",
        "import cv2\n",
        "import psutil\n",
        "import pickle\n",
        "import shutil\n",
        "import os\n",
        "from os import path\n",
        "from matplotlib import pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "%matplotlib inline\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, InputSpec, LeakyReLU, Input, Conv2D, Activation, Concatenate, Conv2DTranspose, BatchNormalization, AveragePooling2D, Add\n",
        "from tensorflow import pad\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.initializers import Constant\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "! pip install tensorflow-addons\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IDgBWEGvLwk0"
      },
      "source": [
        "# Plot functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dgW_xG1LxOB"
      },
      "source": [
        "def plot_grid(images, columns, show_axis=False, labels=None):\n",
        "  if len(images) == 0 or columns <= 0:\n",
        "    return\n",
        "  scale = 2\n",
        "  height = (1 + math.ceil(len(images) / columns) * 2) * scale\n",
        "  width = (columns * 4) * scale\n",
        "  dpi = max(images[0].shape[0], images[0].shape[1]) // 2\n",
        "  fig = plt.figure(figsize=(width, height), dpi=dpi)\n",
        "  fig.subplots_adjust(hspace=0.4)\n",
        "  for index, img in enumerate(images, start=1):\n",
        "    if 'float' in img.dtype.str:\n",
        "      img = (img * 255).astype('uint8')\n",
        "    sp = fig.add_subplot(math.ceil(len(images) / columns), columns, index)\n",
        "    if not show_axis:\n",
        "      plt.axis('off')\n",
        "    if len(np.shape(img)) == 2 or (len(np.shape(img)) > 2 and np.shape(img)[2] == 1):\n",
        "      plt.imshow(img, cmap='gray')\n",
        "    else:\n",
        "      plt.imshow(img)\n",
        "    if labels is not None:\n",
        "      l = len(labels)\n",
        "      sp.set_title(labels[(index-1) % l], fontsize=10)\n",
        "    else:\n",
        "      sp.set_title(index, fontsize=10)\n",
        "  plt.show()\n",
        "\n",
        "def float_to_int_images(outputs):\n",
        "  return [np.clip(output * 255 + 0.5, 0, 255).astype(np.uint8) for output in outputs] "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "za4OJRPRrvTD"
      },
      "source": [
        "# Global parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y9ort4c0Hvkv"
      },
      "source": [
        "# Dimension after the preprocess stage\n",
        "# Should be the dimension expected by the network and the loss functions\n",
        "input_shape = (224, 224, 3)\n",
        "\n",
        "# Batch size used for training and fetching images\n",
        "batch_size = 16\n",
        "\n",
        "# Images are split between train+validation and test set at this proportion\n",
        "validation_split = 0.2\n",
        "\n",
        "epochs_count = 10\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD4x-hbYuR4l"
      },
      "source": [
        "# Dataset loading and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeiQb3KOQd3V"
      },
      "source": [
        "def lambda_generator(batches, λ=lambda x: x):\n",
        "  for batch in batches:\n",
        "    if type(batch) is tuple:\n",
        "      batch, labels = batch\n",
        "      yield [λ(i) for i in batch], labels\n",
        "    else:\n",
        "      yield [λ(i) for i in batch]\n",
        "\n",
        "def random_merge_generator(it_1, it_2, p=0.5):\n",
        "  while True:\n",
        "    rand = np.random.random()\n",
        "    it, other = (it_1, it_2) if rand < p else (it_2, it_1)\n",
        "    try:\n",
        "      yield next(it)\n",
        "    except StopIteration:\n",
        "      while True:\n",
        "        yield next(other)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkr9-p-h9bfm"
      },
      "source": [
        "norm_mean = np.asfarray([0.485, 0.456, 0.406])\n",
        "norm_std = np.asfarray([0.229, 0.224, 0.225])\n",
        "\n",
        "def normalize(img):\n",
        "  return (img - norm_mean) / norm_std\n",
        "def unnormalize(img):\n",
        "  return tf.clip_by_value(img * norm_std + norm_mean, 0.0, 1.0)\n",
        "def rescale_and_normalize(img):\n",
        "  return normalize(img / 255.0)\n",
        "\n",
        "def generated_to_images(outputs):\n",
        "  return [unnormalize(output).numpy() for output in outputs]\n",
        "\n",
        "def test():\n",
        "  a = np.asfarray([[[1.0, 0.5, 0.5], [0.0, 0.1, 0.9]], [[0.5, 0.6, 0.7], [1.0, 0.1, 0.2]]])\n",
        "  b = normalize(a)\n",
        "  c = unnormalize(b)\n",
        "  assert np.linalg.norm(c - a) < 0.00001\n",
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdCRHDcmuWxJ"
      },
      "source": [
        "data_generator_settings = {\n",
        "    'data_format' : 'channels_last',\n",
        "    'validation_split' : validation_split,\n",
        "    'preprocessing_function' : rescale_and_normalize,\n",
        "    #'rescale' : 1.0 / 255,\n",
        "    'horizontal_flip' : True\n",
        "}\n",
        "\n",
        "data_flow_settings = {\n",
        "    'color_mode' : 'rgb',\n",
        "    'batch_size' : batch_size,\n",
        "    'shuffle' : True,\n",
        "    'seed' : 42,\n",
        "    'class_mode' : None,\n",
        "    'interpolation' : 'bilinear',\n",
        "    'target_size' : (input_shape[0], input_shape[1])\n",
        "}\n",
        "\n",
        "def cartoon_real_generator(subset='training'):\n",
        "  cartoon_real_gen = ImageDataGenerator(\n",
        "    **data_generator_settings\n",
        "  )\n",
        "  return cartoon_real_gen.flow_from_directory(\n",
        "        **data_flow_settings,\n",
        "        # Ghibli cartoon\n",
        "        directory = path.join(folders[ghibli_index], real_folder),\n",
        "        subset = subset\n",
        "      )\n",
        "\n",
        "def cartoon_real_validation_generator():\n",
        "  return cartoon_real_generator('validation')\n",
        "\n",
        "def cartoon_smooth_generator(subset='training'):\n",
        "  cartoon_smooth_gen = ImageDataGenerator(\n",
        "    **data_generator_settings\n",
        "  )\n",
        "  return cartoon_smooth_gen.flow_from_directory(\n",
        "        **data_flow_settings,\n",
        "        # Ghibli cartoon\n",
        "        directory = path.join(folders[ghibli_index], smooth_folder),\n",
        "        subset = subset\n",
        "      )\n",
        "  \n",
        "def cartoon_smooth_validation_generator():\n",
        "  return cartoon_smooth_generator('validation')\n",
        "\n",
        "def real_generator(subset='training'):\n",
        "  real_gen = ImageDataGenerator(\n",
        "      **data_generator_settings\n",
        "  )\n",
        "  return real_gen.flow_from_directory(\n",
        "      **data_flow_settings,\n",
        "      # Flickr30k images\n",
        "      directory=flickr30k_folder,\n",
        "      subset=subset\n",
        "  )\n",
        "\n",
        "def real_validation_generator():\n",
        "  return real_generator('validation')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2NBdlQgjxso"
      },
      "source": [
        "gen = cartoon_real_generator()\n",
        "batches_per_epoch = len(gen)\n",
        "print(\"Batches per epoch:\", batches_per_epoch)\n",
        "del gen"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqumAei81Roa"
      },
      "source": [
        "test_cartoon_real_flow = cartoon_real_generator()\n",
        "plot_grid(generated_to_images(next(test_cartoon_real_flow)), 4)\n",
        "del test_cartoon_real_flow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iQrSxTqRX2oM"
      },
      "source": [
        "test_cartoon_edge_fake_flow = cartoon_smooth_generator()\n",
        "plot_grid(generated_to_images(next(test_cartoon_edge_fake_flow)), 4)\n",
        "del test_cartoon_edge_fake_flow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U2w7SPwVHiPl"
      },
      "source": [
        "test_real_flow = real_generator()\n",
        "plot_grid(generated_to_images(next(test_real_flow)), 4)\n",
        "del test_real_flow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6ZfIsl-rzgH"
      },
      "source": [
        "# Cartoon-GAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVdnnqJrr49p"
      },
      "source": [
        "## Custom Convolutional Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgZVztXmxfqh"
      },
      "source": [
        "class ReflectionPadding2D(Layer):\n",
        "  def __init__(self, padding=(1, 1), **kwargs):\n",
        "    self.padding = tuple(padding)\n",
        "    # self.input_spec = [InputSpec(ndim=4)]\n",
        "    super(ReflectionPadding2D, self).__init__(**kwargs)\n",
        "\n",
        "  def compute_output_shape(self, s):\n",
        "    if s[1] == None:\n",
        "      return (None, None, None, s[3])\n",
        "    return (s[0], s[1] + 2 * self.padding[0], s[2] + 2 * self.padding[1], s[3])\n",
        "\n",
        "  def call(self, x, mask=None):\n",
        "    w_pad, h_pad = self.padding\n",
        "    return tf.pad(x, [[0, 0], [h_pad, h_pad], [w_pad, w_pad], [0, 0]], 'REFLECT')\n",
        "\n",
        "  def get_config(self):\n",
        "    config = super(ReflectionPadding2D, self).get_config()\n",
        "    return config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86_eKtrzX8eS"
      },
      "source": [
        "class Conv2DReflection3x3(Layer):\n",
        "  def __init__(self, features, stride=1):\n",
        "    super().__init__()\n",
        "    self.reflectionPadding2D = ReflectionPadding2D()\n",
        "    self.conv2d = Conv2D(features, (3,3), strides=(stride, stride), padding='valid', use_bias=False)\n",
        "\n",
        "  def call(self, inputs, training=False):\n",
        "    x = self.reflectionPadding2D(inputs, training=training)\n",
        "    return self.conv2d(x, training=training)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QosKh8nXwVDy"
      },
      "source": [
        "## Discriminator\n",
        "Based on the Cartoon-GAN discriminator, available at [this link](https://github.com/FilipAndersson245/cartoon-gan/blob/master/models/discriminator.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWKAZhc0TtT8"
      },
      "source": [
        "# define the discriminator model\n",
        "def define_discriminator(image_shape):\n",
        "  alpha = 0.2\n",
        "  epsilon = 1e-5\n",
        "  momentum = 0.1\n",
        "\n",
        "  # source image input\n",
        "  in_image = Input(shape=image_shape)\n",
        "\n",
        "  # k3n32s1\n",
        "  d = Conv2DReflection3x3(32, stride=1)(in_image)\n",
        "  d = LeakyReLU(alpha=alpha)(d)\n",
        "\n",
        "  # k3n64s2\n",
        "  d = Conv2DReflection3x3(64, stride=2)(d)\n",
        "  d = LeakyReLU(alpha=alpha)(d)\n",
        "  # k3n128s1\n",
        "  d = Conv2DReflection3x3(128, stride=1)(d)\n",
        "  d = BatchNormalization(epsilon=epsilon, momentum=momentum)(d)\n",
        "  d = LeakyReLU(alpha=alpha)(d)\n",
        "\n",
        "  # k3n128s2\n",
        "  d = Conv2DReflection3x3(128, stride=2)(d)\n",
        "  d = LeakyReLU(alpha=alpha)(d)\n",
        "  # k3n256s1\n",
        "  d = Conv2DReflection3x3(256, stride=1)(d)\n",
        "  d = BatchNormalization(epsilon=epsilon, momentum=momentum)(d)\n",
        "  d = LeakyReLU(alpha=alpha)(d)\n",
        "\n",
        "  # feature construction block\n",
        "  # k3n256s1\n",
        "  d = Conv2DReflection3x3(256, stride=1)(d)\n",
        "  d = BatchNormalization(epsilon=epsilon, momentum=momentum)(d)\n",
        "  d = LeakyReLU(alpha=alpha)(d)\n",
        "\n",
        "  # patch output\n",
        "  d = Conv2DReflection3x3(1, stride=1)(d)\n",
        "  patch_out = tf.keras.activations.sigmoid(d)\n",
        "\n",
        "  # define model\n",
        "  model = Model(in_image, patch_out, name='Discriminator')\n",
        "  return model\n",
        "\n",
        "D = define_discriminator(input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7aL1dg_7WxKG"
      },
      "source": [
        "#plot_model(D, show_shapes=True, expand_nested=True)\n",
        "D.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z4dAX0fbk3yk"
      },
      "source": [
        "noise = tf.random.normal([1, *input_shape])\n",
        "label_image = D(noise, training=False)\n",
        "\n",
        "plt.imshow(label_image[0, :, :, 0], cmap='gray')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmXCQikgsCVj"
      },
      "source": [
        "## Generator\n",
        "Based on the Cartoon-GAN generator, available at [this link](https://github.com/FilipAndersson245/cartoon-gan/blob/master/models/generator.py)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mTQsEIexgEot"
      },
      "source": [
        "# define the generator model\n",
        "def define_generator(image_shape):\n",
        "  alpha = 0.2\n",
        "  epsilon = 1e-5\n",
        "  momentum = 0.1\n",
        "\n",
        "  # source image input\n",
        "  in_image = Input(shape=image_shape)\n",
        "\n",
        "  # flat block\n",
        "  # k7n64s1\n",
        "  g = Conv2D(64, (7,7), strides=1, padding='same', use_bias=False)(in_image)\n",
        "  g = BatchNormalization(epsilon=epsilon, momentum=momentum)(g)\n",
        "  g = LeakyReLU(alpha=alpha)(g)\n",
        "\n",
        "  def down_block(x, n_features):\n",
        "    # k3n?s2\n",
        "    x = Conv2DReflection3x3(n_features, stride=2)(x)\n",
        "    # k3n?s1\n",
        "    x = Conv2DReflection3x3(n_features, stride=1)(x)\n",
        "    x = BatchNormalization(epsilon=epsilon, momentum=momentum)(x)\n",
        "    x = LeakyReLU(alpha=alpha)(x)\n",
        "    return x\n",
        "\n",
        "  # 1st down block\n",
        "  g = down_block(g, 128)\n",
        "\n",
        "  # 2nd down block\n",
        "  g = down_block(g, 256)\n",
        "\n",
        "  def resiual_block(x):\n",
        "    skip = x\n",
        "    # k3n256s1\n",
        "    x = Conv2DReflection3x3(256, stride=1)(x)\n",
        "    x = BatchNormalization(epsilon=epsilon, momentum=momentum)(x)\n",
        "    x = LeakyReLU(alpha=alpha)(x)\n",
        "    # k3n256s1\n",
        "    x = Conv2DReflection3x3(256, stride=1)(x)\n",
        "    x = BatchNormalization(epsilon=epsilon, momentum=momentum)(x)\n",
        "    x = Add()([x, skip])\n",
        "    x = LeakyReLU(alpha=alpha)(x)\n",
        "    return x\n",
        "\n",
        "  for _ in range(8):\n",
        "    g = resiual_block(g)\n",
        "\n",
        "  def up_block(x, n_features):\n",
        "    # k3n?s1/2\n",
        "    x = Conv2DTranspose(n_features, (3,3), strides=2)(x)\n",
        "    x = AveragePooling2D(pool_size=(2,2), strides=1)(x)\n",
        "    # k3n?s1\n",
        "    x = Conv2DReflection3x3(n_features, stride=1)(x)\n",
        "    x = BatchNormalization(epsilon=epsilon, momentum=momentum)(x)\n",
        "    x = LeakyReLU(alpha=alpha)(x)\n",
        "    return x\n",
        "\n",
        "  # 1st up block\n",
        "  g = up_block(g, 128)\n",
        "\n",
        "  # 2nd up-block\n",
        "  g = up_block(g, 64)\n",
        "\n",
        "  # k7n3s1\n",
        "  output = Conv2D(3, (7,7), strides=1, padding='same')(g)\n",
        "\n",
        "  # define model\n",
        "  model = Model(in_image, output, name='Generator')\n",
        "  return model\n",
        "\n",
        "\n",
        "G = define_generator(input_shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7FJA_a7l72Jc"
      },
      "source": [
        "#plot_model(G, show_shapes=True, expand_nested=True)\n",
        "G.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YeZ-8nTpZLqp"
      },
      "source": [
        "noise = tf.random.normal([1, *input_shape])\n",
        "generated_image = G(noise, training=False)\n",
        "plot_grid(generated_to_images(generated_image), 1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBKihPPW_WkZ"
      },
      "source": [
        "## Loss functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WaHvmumJ6fLL"
      },
      "source": [
        "### Binary Cross Entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1I8Kc6wYRka"
      },
      "source": [
        "def BCEWithLogitsLoss():\n",
        "  return tf.keras.losses.BinaryCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
        "\n",
        "def BCELoss():\n",
        "  return tf.keras.losses.BinaryCrossentropy(\n",
        "    from_logits=False,\n",
        "    reduction=tf.keras.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
        "\n",
        "def test():\n",
        "  a = tf.ones((10, 64))\n",
        "  b = tf.fill((10, 64), 1.5)\n",
        "  loss = BCEWithLogitsLoss()\n",
        "  assert abs(loss(a, b).numpy() - 0.2014133) <= 1e-8\n",
        "\n",
        "  a = tf.ones((10, 64))\n",
        "  loss = BCELoss()\n",
        "  assert abs(loss(a, b).numpy() - 0.0) <= 1e-8\n",
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WL--DGLL6kJa"
      },
      "source": [
        "### Adversarial Loss\n",
        "Also called Discriminator loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e2v-V58ORS95"
      },
      "source": [
        "class AdversarialLoss:\n",
        "  def __init__(self, cartoon_labels, fake_cartoon_labels):\n",
        "    self.base_loss = BCELoss()\n",
        "    self.cartoon_labels = cartoon_labels\n",
        "    self.fake_cartoon_labels = fake_cartoon_labels\n",
        "\n",
        "  def __call__(self, cartoons_outputs, generated_fakes_outputs, cartoon_edge_fakes_outputs):\n",
        "    batch_size = len(cartoons_outputs)\n",
        "    cartoon_labels = tf.stack([self.cartoon_labels for _ in range(batch_size)])\n",
        "    fake_cartoon_labels = tf.stack([self.fake_cartoon_labels for _ in range(batch_size)])\n",
        "    D_cartoon_loss = self.base_loss(cartoons_outputs, cartoon_labels)\n",
        "    D_generated_fake_loss = self.base_loss(generated_fakes_outputs, fake_cartoon_labels) # TODO\n",
        "    D_edge_fake_loss = self.base_loss(cartoon_edge_fakes_outputs, fake_cartoon_labels)\n",
        "\n",
        "    return D_cartoon_loss + D_generated_fake_loss + D_edge_fake_loss\n",
        "\n",
        "# alias for clarity\n",
        "DiscriminatorLoss = AdversarialLoss\n",
        "\n",
        "def test():\n",
        "  loss = AdversarialLoss(np.ones((56, 56)), np.zeros((56, 56)))\n",
        "  cartoon = tf.fill((10, 56, 56), 0.6)\n",
        "  gf = tf.fill((10, 56, 56), 0.4)\n",
        "  cartoon_edge = tf.fill((10, 56, 56), 0.3)\n",
        "  l1 = loss(cartoon, gf, cartoon_edge)\n",
        "\n",
        "  cartoon = tf.fill((10, 56, 56), 0.9)\n",
        "  gf = tf.fill((10, 56, 56), 0.3)\n",
        "  cartoon_edge = tf.fill((10, 56, 56), 0.2)\n",
        "  l2 = loss(cartoon, gf, cartoon_edge)\n",
        "\n",
        "  cartoon = tf.fill((10, 56, 56), 0.99)\n",
        "  gf = tf.fill((10, 56, 56), 0.01)\n",
        "  cartoon_edge = tf.fill((10, 56, 56), 0.01)\n",
        "  l3 = loss(cartoon, gf, cartoon_edge)\n",
        "  assert l1 > l2 > l3 > 0\n",
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKqNjV4K6p2e"
      },
      "source": [
        "### Content Loss\n",
        "Used to force content fidelty in the generator."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zPZxQNSVtr6"
      },
      "source": [
        "from tensorflow.keras.applications import vgg19\n",
        "\n",
        "vgg19_model = vgg19.VGG19(include_top=False, \n",
        "                    weights='imagenet', \n",
        "                    input_shape=input_shape)\n",
        "\n",
        "vgg19_model.trainable = False\n",
        "for l in vgg19_model.layers:\n",
        "  l.trainable = False\n",
        "\n",
        "class ContentLoss:\n",
        "  def __init__(self):\n",
        "    self.perception = lambda img: vgg19_model(vgg19.preprocess_input(unnormalize(img) * 255.0), training=False)\n",
        "  \n",
        "  def __call__(self, outputs, targets):\n",
        "    diff = self.perception(outputs) - self.perception(targets)\n",
        "    k = tf.norm(diff, ord=1)\n",
        "    return k\n",
        "\n",
        "def test():\n",
        "  loss = ContentLoss()\n",
        "  outputs = tf.fill((10, 224, 224, 3), 0.0)\n",
        "  assert loss(outputs, outputs) == 0.0\n",
        "  inputs = tf.fill((10, 224, 224, 3), 1.0)\n",
        "  assert loss(outputs, inputs) > 0.0\n",
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mFJmBsDb67AI"
      },
      "source": [
        "### Generator Loss\n",
        "Enforces both discriminator fooling and content fidelty from the original."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xGF728qWboj"
      },
      "source": [
        "class GeneratorLoss:\n",
        "  def __init__(self, cartoon_labels, omega=10):\n",
        "    self.omega = tf.constant(omega, dtype=tf.float32)\n",
        "    self.content_loss = ContentLoss()\n",
        "    self.base_loss = BCELoss()\n",
        "    self.cartoon_labels = cartoon_labels\n",
        "  \n",
        "  def __call__(self, outputs, inputs, outputs_labels):\n",
        "    batch_size = len(outputs)\n",
        "    cartoon_labels = tf.stack([self.cartoon_labels for _ in range(batch_size)])\n",
        "    return self.base_loss(outputs_labels, cartoon_labels) + self.omega * self.content_loss(outputs, inputs)\n",
        "\n",
        "def test():\n",
        "  loss = GeneratorLoss(tf.ones((56, 56)), omega=100)\n",
        "  outputs_labels = tf.fill((10, 56, 56), 1.0)\n",
        "  outputs = tf.fill((10, 224, 224, 3), 0.0)\n",
        "  assert loss(outputs, outputs, outputs_labels) == 0.0\n",
        "  loss = GeneratorLoss(tf.ones((56, 56)), omega=10)\n",
        "  outputs_labels = tf.fill((10, 56, 56), 0.5)\n",
        "  outputs = tf.fill((10, 224, 224, 3), 0.0)\n",
        "  assert loss(outputs, outputs, outputs_labels) > 0.0\n",
        "  loss = GeneratorLoss(tf.ones((56, 56)), omega=100)\n",
        "  outputs_labels = tf.fill((10, 56, 56), 1.0)\n",
        "  outputs = tf.fill((10, 224, 224, 3), 0.0)\n",
        "  inputs = tf.fill((10, 224, 224, 3), 1.0)\n",
        "  assert loss(outputs, inputs, outputs_labels) > 0.0\n",
        "test()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXXAoGW_Kao_"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O28O5OtYCYUq"
      },
      "source": [
        "class InputIterator(object):\n",
        "\tdef __init__(self, inputs, batch_size=64, shuffle=True, seed=None):\n",
        "\t\tself._inputs = inputs\n",
        "\t\tself._inputs_list = isinstance(inputs, list)\n",
        "\t\tself._N = self._inputs[0].shape[0] if self._inputs_list else self._inputs.shape[0]\n",
        "\t\tself.batch_size = batch_size\n",
        "\t\tself._shuffle = shuffle\n",
        "\t\tself._prng = np.random.RandomState(seed=seed)\n",
        "\t\tself._next_indices = np.array([], dtype=np.uint)\n",
        "\n",
        "\tdef __iter__(self):\n",
        "\t\treturn self\n",
        "\n",
        "\tdef __next__(self):\n",
        "\t\twhile len(self._next_indices) < self.batch_size:\n",
        "\t\t\tnext_ind = np.arange(self._N, dtype=np.uint)\n",
        "\t\t\tif self._shuffle:\n",
        "\t\t\t\tself._prng.shuffle(next_ind)\n",
        "\t\t\tself._next_indices = np.concatenate((\n",
        "\t\t\t\tself._next_indices, next_ind))\n",
        "\n",
        "\t\tind = self._next_indices[:self.batch_size]\n",
        "\t\tself._next_indices = self._next_indices[self.batch_size:]\n",
        "\n",
        "\t\tif self._inputs_list:\n",
        "\t\t\tbatch = [inp[ind,...] for inp in self._inputs]\n",
        "\t\telse:\n",
        "\t\t\tbatch = self._inputs[ind,...]\n",
        "\n",
        "\t\treturn batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhiwdZ4VqJLR"
      },
      "source": [
        "import warnings\n",
        "\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "from keras import backend as K\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def update_mean_cov(mean, cov, N, batch):\n",
        "\tbatch_N = batch.shape[0]\n",
        "\n",
        "\tx = batch\n",
        "\tN += batch_N\n",
        "\tx_norm_old = batch-mean\n",
        "\tmean = mean + x_norm_old.sum(axis=0)/N\n",
        "\tx_norm_new = batch-mean\n",
        "\tcov = ((N-batch_N)/N)*cov + x_norm_old.T.dot(x_norm_new)/N\n",
        "\n",
        "\treturn (mean, cov, N)\n",
        "\n",
        "\n",
        "def frechet_distance(mean1, cov1, mean2, cov2):\n",
        "\t\"\"\"Frechet distance between two multivariate Gaussians.\n",
        "\tArguments:\n",
        "\t\tmean1, cov1, mean2, cov2: The means and covariances of the two\n",
        "\t\t\tmultivariate Gaussians.\n",
        "\tReturns:\n",
        "\t\tThe Frechet distance between the two distributions.\n",
        "\t\"\"\"\n",
        "\t\n",
        "\tdef check_nonpositive_eigvals(l):\n",
        "\t\tnonpos = (l < 0)\n",
        "\t\tif nonpos.any():\n",
        "\t\t\twarnings.warn('Rank deficient covariance matrix, '\n",
        "\t\t\t\t'Frechet distance will not be accurate.', Warning)\n",
        "\t\tl[nonpos] = 0\n",
        "\n",
        "\t(l1,v1) = np.linalg.eigh(cov1)\n",
        "\tcheck_nonpositive_eigvals(l1)\n",
        "\tcov1_sqrt = (v1*np.sqrt(l1)).dot(v1.T)\n",
        "\tcov_prod = cov1_sqrt.dot(cov2).dot(cov1_sqrt)\n",
        "\tlp = np.linalg.eigvalsh(cov_prod)\n",
        "\tcheck_nonpositive_eigvals(lp)\n",
        "\n",
        "\ttrace = l1.sum() + np.trace(cov2) - 2*np.sqrt(lp).sum()\n",
        "\tdiff_mean = mean1-mean2\n",
        "\tfd = diff_mean.dot(diff_mean) + trace\n",
        "\n",
        "\treturn fd\n",
        "\n",
        "\n",
        "class FrechetInceptionDistance(object):\n",
        "\t\"\"\"Frechet Inception Distance.\n",
        "\t\n",
        "\tClass for evaluating Keras-based GAN generators using the Frechet\n",
        "\tInception Distance (Heusel et al. 2017, \n",
        "\thttps://arxiv.org/abs/1706.08500).\n",
        "\tArguments to constructor:\n",
        "\t\tgenerator: a Keras model trained as a GAN generator\n",
        "\t\timage_range: A tuple giving the range of values in the images output\n",
        "\t\t\tby the generator. This is used to rescale to the (-1,1) range\n",
        "\t\t\texpected by the Inception V3 network. \n",
        "\t\tgenerator_postprocessing: A function, preserving the shape of the\n",
        "\t\t\toutput, to be applied to all generator outputs for further \n",
        "\t\t\tpostprocessing. If None (default), no postprocessing will be\n",
        "\t\t\tdone.\n",
        "\tAttributes: The arguments above all have a corresponding attribute\n",
        "\t\twith the same name that can be safely changed after initialization.\n",
        "\tArguments to call:\n",
        "\t\treal_images: An 4D NumPy array of images from the training dataset,\n",
        "\t\t\tor a Python generator outputting training batches. The number of\n",
        "\t\t\tchannels must be either 3 or 1 (in the latter case, the single\n",
        "\t\t\tchannel is distributed to each of the 3 channels expected by the\n",
        "\t\t\tInception network).\n",
        "\t\tgenerator_inputs: One of the following:\n",
        "\t\t\t1. A NumPy array with generator inputs, or\n",
        "\t\t\t2. A list of NumPy arrays (if the generator has multiple inputs)\n",
        "\t\t\t3. A Python generator outputting batches of generator inputs\n",
        "\t\t\t\t(either a single array or a list of arrays)\n",
        "\t\tbatch_size: The size of the batches in which the data is processed.\n",
        "\t\t\tNo effect if Python generators are passed as real_images or\n",
        "\t\t\tgenerator_inputs.\n",
        "\t\tnum_batches_real: Number of batches to use to evaluate the mean and\n",
        "\t\t\tthe covariance of the real samples.\n",
        "\t\tnum_batches_gen: Number of batches to use to evaluate the mean and\n",
        "\t\t\tthe covariance of the generated samples. If None (default), set\n",
        "\t\t\tequal to num_batches_real.\n",
        "\t\tshuffle: If True (default), samples are randomly selected from the\n",
        "\t\t\tinput arrays. No effect if real_images or generator_inputs is\n",
        "\t\t\ta Python generator.\n",
        "\t\tseed: A random seed for shuffle (to provide reproducible results)\n",
        "\tReturns (call):\n",
        "\t\tThe Frechet Inception Distance between the real and generated data.\n",
        "\t\"\"\"\n",
        "\tdef __init__(self, generator, image_range, \n",
        "\t\tgenerator_postprocessing=None):\n",
        "\n",
        "\t\tself._inception_v3 = None\n",
        "\t\tself.generator = generator\n",
        "\t\tself.generator_postprocessing = generator_postprocessing\n",
        "\t\tself.image_range = image_range\n",
        "\t\tself._channels_axis = \\\n",
        "\t\t\t-1 if K.image_data_format()==\"channels_last\" else -3\n",
        "\n",
        "\tdef _setup_inception_network(self):\n",
        "\t\tself._inception_v3 = InceptionV3(\n",
        "\t\t\tinclude_top=False, pooling='avg')\n",
        "\t\tself._pool_size = self._inception_v3.output_shape[-1]\n",
        "\n",
        "\tdef _preprocess(self, images):\n",
        "\t\tif self.image_range != (-1,1):\n",
        "\t\t\timages = images - self.image_range[0]\n",
        "\t\t\timages /= (self.image_range[1]-self.image_range[0])/2.0\n",
        "\t\t\timages -= 1.0\n",
        "\t\tif images.shape[self._channels_axis] == 1:\n",
        "\t\t\timages = np.concatenate([images]*3, axis=self._channels_axis)\n",
        "\t\treturn images\n",
        "\n",
        "\tdef _stats(self, inputs, input_type=\"real\", postprocessing=None,\n",
        "\t\tbatch_size=64, num_batches=128, shuffle=True, seed=None):\n",
        "\n",
        "\t\tmean = np.zeros(self._pool_size)\n",
        "\t\tcov = np.zeros((self._pool_size,self._pool_size))\n",
        "\t\tN = 0\n",
        "\n",
        "\t\tfor i in range(num_batches):\n",
        "\t\t\ttry:\n",
        "\t\t\t\t# draw a batch from generator input iterator\n",
        "\t\t\t\tbatch = next(inputs)\n",
        "\t\t\texcept TypeError:\n",
        "\t\t\t\t# assume that an array or a list of arrays was passed\n",
        "\t\t\t\t# instead\n",
        "\t\t\t\tinputs = InputIterator(inputs,\n",
        "\t\t\t\t\tbatch_size=batch_size, shuffle=shuffle, seed=seed)\n",
        "\t\t\t\tbatch = next(inputs)\n",
        "\n",
        "\t\t\tif input_type==\"generated\":\n",
        "\t\t\t\tbatch = self.generator.predict(batch)\n",
        "\t\t\tif postprocessing is not None:\n",
        "\t\t\t\tbatch = postprocessing(batch)\n",
        "\t\t\tbatch = self._preprocess(batch)\n",
        "\t\t\tpool = self._inception_v3.predict(batch, batch_size=batch_size)\n",
        "\n",
        "\t\t\t(mean, cov, N) = update_mean_cov(mean, cov, N, pool)\n",
        "\n",
        "\t\treturn (mean, cov)\n",
        "\n",
        "\tdef __call__(self,\n",
        "\t\t\treal_images,\n",
        "\t\t\tgenerator_inputs,\n",
        "\t\t\tbatch_size=64,\n",
        "\t\t\tnum_batches_real=128,\n",
        "\t\t\tnum_batches_gen=None,\n",
        "\t\t\tshuffle=True,\n",
        "\t\t\tseed=None\n",
        "\t\t):\n",
        "\n",
        "\t\tif self._inception_v3 is None:\n",
        "\t\t\tself._setup_inception_network()\n",
        "\n",
        "\t\t(real_mean, real_cov) = self._stats(real_images,\n",
        "\t\t\t\"real\", batch_size=batch_size, num_batches=num_batches_real,\n",
        "\t\t\tshuffle=shuffle, seed=seed)\n",
        "\t\tif num_batches_gen is None:\n",
        "\t\t\tnum_batches_gen = num_batches_real\n",
        "\t\t(gen_mean, gen_cov) = self._stats(generator_inputs,\n",
        "\t\t\t\"generated\", batch_size=batch_size, num_batches=num_batches_gen,\n",
        "\t\t\tpostprocessing=self.generator_postprocessing,\n",
        "\t\t\tshuffle=shuffle, seed=seed)\n",
        "\n",
        "\t\treturn frechet_distance(real_mean, real_cov, gen_mean, gen_cov)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUfoGjpjsM7i"
      },
      "source": [
        "# fd = FrechetInceptionDistance(G, (0, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMKAEyrEsiBD"
      },
      "source": [
        "# cartoon = next(crg)\n",
        "# smooth = next(csg)\n",
        "# fd(cartoon_real_validation_generator(), cartoon_real_validation_generator(), shuffle=False, num_batches_real=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqWyxfkm9q5f"
      },
      "source": [
        "learning_rate = 1.5e-4\n",
        "beta1, beta2 = (.5, .99)\n",
        "weight_decay = 1e-4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "309mSNxD9W1d"
      },
      "source": [
        "discriminator_optimizer = tfa.optimizers.AdamW(\n",
        "    learning_rate=learning_rate, \n",
        "    beta_1=beta1, beta_2=beta2,\n",
        "    weight_decay=weight_decay\n",
        ")\n",
        "generator_optimizer = tfa.optimizers.AdamW(\n",
        "    learning_rate=learning_rate, \n",
        "    beta_1=beta1, beta_2=beta2,\n",
        "    weight_decay=weight_decay\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Chnai6eyRLSp"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmovpB7LZzKe"
      },
      "source": [
        "### Checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mpmVG5PZy8O"
      },
      "source": [
        "local_checkpoint_location = './training_checkpoints/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_mpWK3Gp4mK"
      },
      "source": [
        "#### Google Drive checkpoint backup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MopeyfiEp-C7"
      },
      "source": [
        "use_google_drive = False #@param {type:'boolean'}\n",
        "google_drive_checkpoint_path = 'Anime-Frames/Checkpoints' #@param {type: 'string'}\n",
        "reset_checkpoints = True #@param {type: 'boolean'}\n",
        "google_drive_root = '/content/drive/'\n",
        "google_drive_checkpoint_location = path.join(google_drive_root, 'MyDrive', google_drive_checkpoint_path)\n",
        "\n",
        "if use_google_drive:\n",
        "  from google.colab import drive\n",
        "  drive.mount(google_drive_root)\n",
        "  os.makedirs(google_drive_checkpoint_location, exist_ok=True)\n",
        "  shutil.rmtree(local_checkpoint_location, ignore_errors=True)\n",
        "  shutil.copytree(google_drive_checkpoint_location, local_checkpoint_location)\n",
        "  print(f'Local files at {local_checkpoint_location} will be backed inside {google_drive_checkpoint_location}')\n",
        "else:\n",
        "  try:\n",
        "    drive.flush_and_unmount()\n",
        "    !rm -rf /content/drive\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "\n",
        "if reset_checkpoints:\n",
        "  shutil.rmtree(local_checkpoint_location, ignore_errors=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGZeeqabwaTD"
      },
      "source": [
        "#### Checkpoint manager"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P4bip6SkwdGp"
      },
      "source": [
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=G,\n",
        "                                 discriminator=D,\n",
        "                                 epoch=tf.Variable(0))\n",
        "checkpoint_manager = tf.train.CheckpointManager(checkpoint, local_checkpoint_location, max_to_keep=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hx-ymhLjeROc"
      },
      "source": [
        "### Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bn4o5b5ILJM",
        "cellView": "code"
      },
      "source": [
        "class CartoonGAN:\n",
        "  def __init__(self,\n",
        "               checkpoint,\n",
        "               checkpoint_manager,\n",
        "               cartoon_real_generator,\n",
        "               cartoon_smooth_generator,\n",
        "               real_image_generator,\n",
        "               cartoon_real_generator_val,\n",
        "               cartoon_smooth_generator_val,\n",
        "               real_image_generator_val):\n",
        "    self.name = 'Cartoon-GAN'\n",
        "    self.checkpoint = checkpoint\n",
        "    self.checkpoint_manager = checkpoint_manager\n",
        "\n",
        "    # Checkpoint restore\n",
        "    checkpoint.restore(checkpoint_manager.latest_checkpoint)\n",
        "    if checkpoint_manager.latest_checkpoint:\n",
        "      print(f'Restored from {checkpoint_manager.latest_checkpoint}.')\n",
        "    else:\n",
        "      print('Initializing from scratch.')\n",
        "    self.discriminator = checkpoint.discriminator\n",
        "    self.generator = checkpoint.generator\n",
        "    self.d_optimizer = checkpoint.discriminator_optimizer\n",
        "    self.g_optimizer = checkpoint.generator_optimizer\n",
        "\n",
        "    self.cartoon_real_generator = cartoon_real_generator # cartoon real gen train\n",
        "    self.cartoon_smooth_generator = cartoon_smooth_generator # cartoon smooth gen train\n",
        "    self.cartoon_real_generator_val = cartoon_real_generator_val # cartoon real gen val\n",
        "    self.cartoon_smooth_generator_val = cartoon_smooth_generator_val # cartoon smooth gen val\n",
        "    self.real_image_generator = real_image_generator # real image gen train\n",
        "    self.real_image_generator_val = real_image_generator_val # real image gen val \n",
        "    \n",
        "    self.batches_per_epoch = len(self.cartoon_real_generator) - 1 # Last batch has unknow size\n",
        "    self.val_batches_per_epoch = len(self.cartoon_real_generator_val) - 1 # Last batch has unknow size\n",
        "\n",
        "    self.cartoon_labels = tf.ones((*D.compute_output_shape((None, *input_shape))[1:],))\n",
        "    self.fake_cartoon_labels = tf.zeros((*D.compute_output_shape((None, *input_shape))[1:],))\n",
        "\n",
        "    self.d_loss_fn = DiscriminatorLoss(self.cartoon_labels, self.fake_cartoon_labels)\n",
        "    self.g_loss_fn = GeneratorLoss(self.cartoon_labels)\n",
        "    self.bar_format = '{bar}{desc}: {percentage:3.0f}% {r_bar}'\n",
        "  \n",
        "  @tf.function\n",
        "  def _step(self,\n",
        "              x_cartoon_batch_train,\n",
        "              x_cartoon_smooth_batch_train,\n",
        "              x_real_train):\n",
        "    \n",
        "    generated_real_train = self.generator(x_real_train, training=False)\n",
        "\n",
        "    with tf.GradientTape() as disc_tape:  \n",
        "      # Discriminator loss\n",
        "      predictions_cartoon_train = self.discriminator(x_cartoon_batch_train, training=True)\n",
        "      predictions_cartoon_smooth_train = self.discriminator(x_cartoon_smooth_batch_train, training=True)\n",
        "      predictions_generated_train = self.discriminator(generated_real_train, training=True)\n",
        "      d_loss = self.d_loss_fn(predictions_cartoon_train, predictions_generated_train, predictions_cartoon_smooth_train)\n",
        "    \n",
        "    gradients_of_discriminator = disc_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "    self.d_optimizer.apply_gradients(\n",
        "        zip(gradients_of_discriminator, self.discriminator.trainable_variables)\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as gen_tape:\n",
        "      generated_real_train = self.generator(x_real_train, training=True)\n",
        "      # Generator loss\n",
        "      generated_real_labels = self.discriminator(generated_real_train, training=False)\n",
        "      g_loss = self.g_loss_fn(generated_real_train, x_real_train, generated_real_labels)\n",
        "    \n",
        "    gradients_of_generator = gen_tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "    self.g_optimizer.apply_gradients(\n",
        "        zip(gradients_of_generator, self.generator.trainable_variables)\n",
        "    )\n",
        "\n",
        "    return d_loss, g_loss\n",
        "\n",
        "  @tf.function\n",
        "  def _val_step(self,\n",
        "              x_cartoon_batch_val,\n",
        "              x_cartoon_smooth_batch_val,\n",
        "              x_real_val):\n",
        "\n",
        "    generated_real_val = self.generator(x_real_val, training=False)\n",
        "    \n",
        "    # Discriminator loss\n",
        "    predictions_cartoon_val = self.discriminator(x_cartoon_batch_val, training=False)\n",
        "    predictions_cartoon_smooth_val = self.discriminator(x_cartoon_smooth_batch_val, training=False)\n",
        "    predictions_generated_val = self.discriminator(generated_real_val, training=False)\n",
        "    d_loss = self.d_loss_fn(predictions_cartoon_val, predictions_generated_val, predictions_cartoon_smooth_val)\n",
        "\n",
        "    # Generator loss (could have been computed from discriminator after one step of training)\n",
        "    generated_real_labels = self.discriminator(generated_real_val, training=False)\n",
        "    g_loss = self.g_loss_fn(generated_real_val, x_real_val, generated_real_labels)\n",
        "\n",
        "    return d_loss, g_loss\n",
        "\n",
        "  def train(self, epochs):\n",
        "    d_losses = []\n",
        "    g_losses = []\n",
        "    starting_epoch = int(self.checkpoint.epoch)\n",
        "    for epoch in tqdm(range(starting_epoch, epochs),\n",
        "                      total=epochs,\n",
        "                      initial=starting_epoch,\n",
        "                      position=0,\n",
        "                      desc=f'Training model {self.name}',\n",
        "                      bar_format=self.bar_format):\n",
        "\n",
        "      # train phase\n",
        "      epoch_progress = tqdm(range(self.batches_per_epoch), \n",
        "                            total=self.batches_per_epoch, \n",
        "                            position=1,\n",
        "                            desc=f'Training epoch {epoch}',\n",
        "                            bar_format=self.bar_format)\n",
        "      for (step,\n",
        "          x_cartoon_batch_train, \n",
        "          x_cartoon_smooth_batch_train, \n",
        "          x_real_train) in zip(\n",
        "             epoch_progress,\n",
        "             self.cartoon_real_generator, \n",
        "             self.cartoon_smooth_generator,\n",
        "             self.real_image_generator):\n",
        "        d_loss, g_loss = self._step(x_cartoon_batch_train,\n",
        "                                    x_cartoon_smooth_batch_train,\n",
        "                                    x_real_train)\n",
        "        epoch_progress.set_postfix({\n",
        "          'Discriminator Loss' : f'{d_loss.numpy():.4f}',\n",
        "          'Generator Loss' : f'{g_loss.numpy():.4f}'\n",
        "        })\n",
        "\n",
        "      # validation phase\n",
        "      epoch_val_progress = tqdm(total=self.val_batches_per_epoch,\n",
        "                                position=1,\n",
        "                                desc=f'Validation epoch {epoch}',\n",
        "                                bar_format=self.bar_format)\n",
        "\n",
        "      d_val_loss_sum, d_val_loss_count = 0, 0\n",
        "      g_val_loss_sum, g_val_loss_count = 0, 0\n",
        "      for crgv, csgv, rigv, _ in zip(self.cartoon_real_generator_val, self.cartoon_smooth_generator_val, self.real_image_generator_val, range(self.val_batches_per_epoch)):\n",
        "        d_val_loss, g_val_loss = self._val_step(crgv, csgv, rigv)\n",
        "        d_val_loss_sum += d_val_loss\n",
        "        d_val_loss_count += 1\n",
        "        g_val_loss_sum += g_val_loss\n",
        "        g_val_loss_count += 1\n",
        "        epoch_val_progress.update(1)\n",
        "      \n",
        "      epoch_val_progress.set_postfix({\n",
        "        'Discriminator Loss' : f'{d_val_loss_sum/d_val_loss_count:.4f}',\n",
        "        'Generator Loss' : f'{g_val_loss_sum/g_val_loss_count:.4f}'\n",
        "      })\n",
        "      epoch_val_progress.close()\n",
        "      \n",
        "      # checkpoint phase\n",
        "      epoch_checkpoint_progress = tqdm(total=2 if use_google_drive else 1,\n",
        "                                 position=1,\n",
        "                                 desc=f'Saving model checkpoints for epoch {epoch}',\n",
        "                                 bar_format=self.bar_format)\n",
        "      self.checkpoint.epoch.assign_add(1)\n",
        "      save_path = self.checkpoint_manager.save()\n",
        "      epoch_checkpoint_progress.update(1)\n",
        "      epoch_checkpoint_progress.set_description(f'Saved checkpoint for epoch {epoch}: {save_path}')\n",
        "      \n",
        "      if use_google_drive:\n",
        "        shutil.rmtree(google_drive_checkpoint_location)\n",
        "        shutil.copytree(self.checkpoint_manager.directory, google_drive_checkpoint_location)\n",
        "        epoch_checkpoint_progress.set_description(f'Saved checkpoint backup for epoch {epoch}: {save_path} -> {google_drive_checkpoint_location}')\n",
        "        epoch_checkpoint_progress.update(1)\n",
        "      epoch_checkpoint_progress.close()\n",
        "  # TODO make a return with losses for history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GQa7MRDmgs1"
      },
      "source": [
        "model = CartoonGAN(checkpoint,\n",
        "                   checkpoint_manager,\n",
        "                   cartoon_real_generator(), \n",
        "                   cartoon_smooth_generator(), \n",
        "                   real_generator(),\n",
        "                   cartoon_real_validation_generator(),\n",
        "                   cartoon_smooth_validation_generator(),\n",
        "                   real_validation_generator())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwmOwg7TeYuq"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DVSlPFOQTaj5"
      },
      "source": [
        "# d_losses, g_losses = \n",
        "model.train(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eNQ_AYEl1Yy"
      },
      "source": [
        "# tf.config.run_functions_eagerly(False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vhoDyZbj86r"
      },
      "source": [
        "# a, b, c = next(cartoon_real_generator()), next(cartoon_smooth_generator()), next(real_generator())\n",
        "# print(type(a))\n",
        "\n",
        "\n",
        "# graph = model._step.get_concrete_function(a, b, c).graph\n",
        "# for node in graph.as_graph_def().node:\n",
        "#   print(f'{node.input} -> {node.name}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4YsZWLrW5sT"
      },
      "source": [
        "cartoon_images = next(cartoon_real_generator())\n",
        "cartoon_smoothed_images = next(cartoon_smooth_generator())\n",
        "black_images = np.full((2, 224, 224, 3), 0.0)\n",
        "\n",
        "print('cartoon')\n",
        "plot_grid(D(cartoon_images).numpy().squeeze(), 4)\n",
        "print('smoothed')\n",
        "plot_grid(D(cartoon_smoothed_images).numpy().squeeze(), 4)\n",
        "print('black')\n",
        "plot_grid(D(black_images).numpy().squeeze(), 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5orBaqsRsCA"
      },
      "source": [
        "real_images = next(real_generator())\n",
        "black_images = np.full((4, 224, 224, 3), 0.0)\n",
        "\n",
        "print('real images')\n",
        "plot_grid(generated_to_images(real_images), 4)\n",
        "print('cartoon generated')\n",
        "plot_grid(generated_to_images(G(real_images)), 4)\n",
        "print('black')\n",
        "plot_grid(generated_to_images(G(black_images)), 4)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}